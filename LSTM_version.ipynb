{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26345,
     "status": "ok",
     "timestamp": 1596195399442,
     "user": {
      "displayName": "narges farrokhshad",
      "photoUrl": "",
      "userId": "09411964733981171008"
     },
     "user_tz": -270
    },
    "id": "HcsV3XIp3dKY",
    "outputId": "2ebd4717-aeac-4407-8a33-858f35fe78b6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2011,
     "status": "ok",
     "timestamp": 1596195401461,
     "user": {
      "displayName": "narges farrokhshad",
      "photoUrl": "",
      "userId": "09411964733981171008"
     },
     "user_tz": -270
    },
    "id": "aalWwhfh21f4"
   },
   "outputs": [],
   "source": [
    "def train_step(u_batch, i_batch, uid, iid, y_batch,batch_num):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    feed_dict = {                         #placeholder haro meghdardehi sabet mikone\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_i: i_batch,\n",
    "        deep.input_uid: uid,\n",
    "        deep.input_iid: iid,\n",
    "        deep.input_y: y_batch,\n",
    "    }\n",
    "    _, step,summaries, loss, RMSE, MAE, fm,prediction,y = sess.run(\n",
    "        [train_op, global_step,train_summary_op, deep.loss, deep.RMSE, deep.MAE,deep.score,deep.predictions,deep.input_y]\n",
    "        , feed_dict)\n",
    "     \n",
    "    # time_str = datetime.datetime.now().strftime(\"%X\")\n",
    "    # print(\"{}: step {}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, batch_num, loss, RMSE, MAE))\n",
    "    train_summary_writer.add_summary(summaries, step)\n",
    "    return RMSE, MAE,fm,prediction,y\n",
    "  \n",
    "def dev_step(u_batch, i_batch, uid, iid, y_batch,writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_i: i_batch,\n",
    "        deep.input_y: y_batch,\n",
    "        deep.input_uid: uid,\n",
    "        deep.input_iid: iid,\n",
    "    }\n",
    "    step,summaries, loss, RMSE, mae,prediction,y = sess.run(\n",
    "        [global_step,dev_summary_op, deep.loss, deep.RMSE, deep.MAE,deep.predictions,deep.input_y], feed_dict)\n",
    "    # time_str = datetime.datetime.now().strftime(\"%X\")\n",
    "#     print(\"{}: step{}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, step, loss, accuracy, mae))\n",
    "    if writer:\n",
    "        writer.add_summary(summaries, step)\n",
    "    return [loss, RMSE, mae,prediction,y]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7240,
     "status": "ok",
     "timestamp": 1596195406699,
     "user": {
      "displayName": "narges farrokhshad",
      "photoUrl": "",
      "userId": "09411964733981171008"
     },
     "user_tz": -270
    },
    "id": "aI1GXlJe246A",
    "outputId": "a2acf273-ab90-40a7-e955-393dc44718b6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import datetime\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "word2vec=\"/content/drive/My Drive/dataset/glove.6B.100d.txt\"\n",
    "valid_data=\"/content/drive/My Drive/dataset/music/project/music.test\"\n",
    "para_data= \"/content/drive/My Drive/dataset/music/project/music.para\"\n",
    "train_data= \"/content/drive/My Drive/dataset/music/project/music.train\"\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim= 100\n",
    "l2_reg_lambda= 0.001\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement= True\n",
    "log_device_placement= False\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# with open (para_data, 'rb') as pkl_file:\n",
    "#   pkl_file.close()\n",
    "pkl_file = open(para_data, 'rb')\n",
    "\n",
    "  \n",
    "para = pickle.load(pkl_file)\n",
    "user_num = para['user_num']\n",
    "item_num = para['item_num']\n",
    "review_num_u = para['review_num_u']\n",
    "review_num_i = para['review_num_i']\n",
    "review_len_u = para['review_len_u']\n",
    "review_len_i = para['review_len_i']\n",
    "vocabulary_user = para['user_vocab']\n",
    "vocabulary_item = para['item_vocab']\n",
    "train_length = para['train_length']\n",
    "test_length = para['test_length']\n",
    "u_text = para['u_text']\n",
    "i_text = para['i_text']\n",
    "\n",
    "np.random.seed(2017)\n",
    "random_seed = 2017\n",
    "print (user_num)\n",
    "print (item_num)\n",
    "print (review_num_u)\n",
    "print (review_len_u)\n",
    "print (review_num_i)\n",
    "print (review_len_i)\n",
    "\n",
    "pkl_file = open(train_data, 'rb')\n",
    "\n",
    "train_data = pickle.load(pkl_file)\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open(valid_data, 'rb')\n",
    "\n",
    "test_data = pickle.load(pkl_file)\n",
    "test_data = np.array(test_data)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16608,
     "status": "ok",
     "timestamp": 1596195416077,
     "user": {
      "displayName": "narges farrokhshad",
      "photoUrl": "",
      "userId": "09411964733981171008"
     },
     "user_tz": -270
    },
    "id": "6zoU8CLX28AA",
    "outputId": "d928b3ec-4ffe-4ffc-8a3d-98b6cf373452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in vocab_u  9545\n",
      "in vocab_i  10189\n"
     ]
    }
   ],
   "source": [
    "u=0\n",
    "v=0\n",
    "initW1 = np.random.uniform(-1.0, 1.0, (len(vocabulary_user), embedding_dim))\n",
    "\n",
    "with open(word2vec,encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        p=line.split()\n",
    "        word=p[0]\n",
    "        if word in vocabulary_user:\n",
    "            u = u + 1\n",
    "            idx = vocabulary_user[word]\n",
    "            initW1[idx] = np.asarray(p[1:], dtype=\"float32\")\n",
    "initW2 = np.random.uniform(-1.0, 1.0, (len(vocabulary_item),embedding_dim))\n",
    "\n",
    "with open(word2vec,encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        p=line.split()\n",
    "        word=p[0]\n",
    "        if word in vocabulary_item:\n",
    "            v=v+1\n",
    "            idx = vocabulary_item[word]\n",
    "            initW2[idx] = np.asarray(p[1:], dtype=\"float32\")\n",
    "                        \n",
    "print (\"in vocab_u \", u)\n",
    "print (\"in vocab_i \", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16600,
     "status": "ok",
     "timestamp": 1596195416078,
     "user": {
      "displayName": "narges farrokhshad",
      "photoUrl": "",
      "userId": "09411964733981171008"
     },
     "user_tz": -270
    },
    "id": "rzIIAROC3AOl"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import tensorflow as tff\n",
    "class NARRE(object):\n",
    "    def __init__(\n",
    "            self, review_num_u, review_num_i, review_len_u, review_len_i, user_num, item_num, num_classes,\n",
    "            user_vocab_size, item_vocab_size, n_latent, embedding_id, attention_size,\n",
    "            embedding_size, l2_reg_lambda=0.1):\n",
    "        self.input_u = tf.placeholder(tf.int32, [None, review_num_u, review_len_u], name=\"input_u\")\n",
    "        self.input_i = tf.placeholder(tf.int32, [None, review_num_i, review_len_i], name=\"input_i\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "        self.input_uid = tf.placeholder(tf.int32, [None, 1], name=\"input_uid\")\n",
    "        self.input_iid = tf.placeholder(tf.int32, [None, 1], name=\"input_iid\")\n",
    "\n",
    "        print('input user shape:',self.input_u.shape)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        self.dropout_keep_prob=tf.constant(0.7)\n",
    "    #############################################\n",
    "        with tf.name_scope(\"user_embedding\"):\n",
    "            self.W1 = tf.Variable(tf.random_uniform([user_vocab_size, embedding_size], -1.0, 1.0),name=\"W1\",trainable=False)\n",
    "            self.embedded_user = tf.nn.embedding_lookup(self.W1, self.input_u)                            #(100,16,441,50)\n",
    "            print('self.embedded_users.shape',self.embedded_user.shape)\n",
    "\n",
    "        with tf.name_scope(\"item_embedding\"):\n",
    "            self.W2 = tf.Variable(tf.random_uniform([item_vocab_size, embedding_size], -1.0, 1.0),name=\"W2\",trainable=False)\n",
    "            self.embedded_item = tf.nn.embedding_lookup(self.W2, self.input_i)\n",
    "            print('self.embedded_item.shape',self.embedded_item.shape)\n",
    "   #######################################################         \n",
    "        with tf.name_scope(\"LSTM_cell\"):\n",
    "            rnn_size=100\n",
    "            num_layers=1\n",
    "        with tf.variable_scope(\"scope\", reuse = False ): \n",
    "            \n",
    "            rnn_cell1 = tf.compat.v1.nn.rnn_cell.LSTMCell(rnn_size,name=\"rnn_user\",reuse = tf.AUTO_REUSE)\n",
    "        with tf.variable_scope(\"scope2\",reuse = False): \n",
    "            rnn_cell2 = tf.compat.v1.nn.rnn_cell.LSTMCell(rnn_size,name=\"rnn_item\",reuse = tf.AUTO_REUSE)\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"LSTM_User\"):\n",
    "            user_jadid=tf.reshape(self.embedded_user,[-1, review_len_u, embedding_size])\n",
    "            print('user jadid shape: ',user_jadid.shape)\n",
    "            self.rnn_outputs_user, user_state = tf.nn.dynamic_rnn(\n",
    "                cell=rnn_cell1, inputs=user_jadid , dtype=user_jadid.dtype)\n",
    "            \n",
    "            self.rnn_outputs_user = tf.nn.dropout(self.rnn_outputs_user, rate=1-self.dropout_keep_prob)   # khorooji CNN user _ feature vector for  every user  - O_ul   (100,16,100)\n",
    "            self.h_pool_flat_u = tf.reshape(self.rnn_outputs_user, [-1, review_num_u*review_len_u, rnn_size])  \n",
    "\n",
    "        with tf.name_scope(\"LSTM_item\"):\n",
    "            item_jadid=tf.reshape(self.embedded_item,[-1, review_len_i, embedding_size])\n",
    "            print('item jadid shape: ',item_jadid.shape)\n",
    "            self.rnn_outputs_item, item_state = tf.nn.dynamic_rnn(\n",
    "                cell=rnn_cell2, inputs=item_jadid ,dtype=item_jadid.dtype)\n",
    "            self.rnn_outputs_item = tf.nn.dropout(self.rnn_outputs_item, rate=1-self.dropout_keep_prob)   #khorooji CNN item _ feature vector for every item        with tf.name_scope(\"dropout_item\"):    \n",
    "            self.h_pool_flat_i = tf.reshape(self.rnn_outputs_item, [-1, review_num_i*review_len_i, rnn_size])  \n",
    "\n",
    "            # stacked = tf.nn.rnn_cell.MultiRNNCell(fw_cells, state_is_tuple=True)\n",
    "            # self.rnn_outputs_user, _ = tf.nn.dynamic_rnn(\n",
    "            #     cell=stacked, inputs=user_jadid , dtype=self.embedded_user.dtype)\n",
    "            # print('a')\n",
    "            # self.rnn_outputs_item, _ = tf.nn.dynamic_rnn(\n",
    "            #     cell=stacked, inputs=item_jadid ,dtype=self.embedded_item.dtype)\n",
    "\n",
    "            print('rnn_outputs_user shape',(self.rnn_outputs_user[0].shape,self.rnn_outputs_user[1].shape))\n",
    "            print('rnn_outputs_item shape',(self.rnn_outputs_item[0].shape,self.rnn_outputs_item[1].shape))\n",
    "#######################################################################\n",
    "\n",
    "        # with tf.name_scope(\"dropout_user\"):\n",
    "\n",
    "        print('user_dropOut',self.rnn_outputs_user.shape) \n",
    "        print('item_dropOut',self.rnn_outputs_item.shape)\n",
    "\n",
    "        print('flat output user:',self.h_pool_flat_u.shape )  \n",
    "        print('flat output item:',self.h_pool_flat_i.shape )\n",
    "###################################### ##########################           \n",
    "        with tf.name_scope(\"attention_user\"): \n",
    "            Wau = tf.Variable(tf.random_uniform([rnn_size, attention_size], -0.1, 0.1), name='Wau')  #Wo\n",
    "            Wpu = tf.Variable(tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpu')  #hT\n",
    "            bau = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bau\")    #b1\n",
    "            bbu = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbu\")    #b2\n",
    "            self.u_j=tf.matmul(tf.nn.relu(tf.matmul(self.h_pool_flat_u, Wau)+ bau),Wpu)+bbu  # None*u_len*1   a*_ul  (100,16,1)\n",
    "\n",
    "            self.attention_score_user = tf.nn.softmax(self.u_j,1,name=\"attention_score_user\")  # none*u_len*1    vazne atention user   a_ul  (100,16,1)\n",
    "            print('attention_score_user',self.attention_score_user.shape)\n",
    "           \n",
    "        with tf.name_scope(\"attention_item\"):  \n",
    "\n",
    "            Wai = tf.Variable( tf.random_uniform([rnn_size, attention_size], -0.1, 0.1), name='Wai')\n",
    "            Wpi = tf.Variable( tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpi')\n",
    "            bai = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bai\")\n",
    "            bbi = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbi\")\n",
    "           \n",
    "            self.i_j=tf.matmul(tf.nn.relu(tf.matmul(self.h_pool_flat_i, Wai)+ bai),Wpi)+bbi\n",
    "\n",
    "            self.attention_score_item = tf.nn.softmax(self.i_j,1,name=\"attention_score_item\")  # none*len*1     # vazne attention item   contribution of the lth review to the feature profile of item i:\n",
    "            print('attention_score_item',self.attention_score_item.shape)\n",
    "\n",
    "###################################################################\n",
    "\n",
    "        with tf.name_scope(\"rated_review_user\"):\n",
    "            self.u_feast = tf.reduce_sum(tf.multiply(self.attention_score_user, self.h_pool_flat_u), 1)\n",
    "            self.u_feas = tf.nn.dropout(self.u_feast, rate=1-self.dropout_keep_prob,name=\"rated_review_user\")  # feature vector for user - user representation (Xu)\n",
    "            \n",
    "            print('user_feature vector:',self.u_feas.shape)\n",
    "        with tf.name_scope(\"rated_review_item\"):\n",
    "            self.i_feas = tf.reduce_sum(tf.multiply(self.attention_score_item, self.h_pool_flat_i), 1)\n",
    "            self.i_feas = tf.nn.dropout(self.i_feas, rate=1-self.dropout_keep_prob,name=\"rated_review_item\")  # feature vector for item - item representation (Yi)\n",
    "            print('item_feature vector:',self.i_feas.shape)\n",
    " ###################################################################           \n",
    "        with tf.name_scope(\"feature_user\"):\n",
    "            uidmf = tf.Variable(tf.random_uniform([51764 + 2, embedding_id], -0.1, 0.1), name=\"uidmf\")\n",
    "\n",
    "            self.uid = tf.nn.embedding_lookup(uidmf,self.input_uid)   # user prefrences (q_u)  \n",
    "            self.uid = tf.reshape(self.uid,[-1,embedding_id])\n",
    "            Wu = tf.Variable( tf.random_uniform([rnn_size, n_latent], -0.1, 0.1), name='Wu')  #100*32\n",
    "            bu = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bu\")\n",
    "            self.u_feas = tf.matmul(self.u_feas, Wu)+self.uid + bu    #Xu+qu\n",
    "            self.u_feas = tf.identity(self.u_feas, name=\"feature_user\")\n",
    "\n",
    "            print('Xu+qu',self.u_feas.shape)\n",
    "        with tf.name_scope(\"feature_item\"):\n",
    "            iidmf = tf.Variable(tf.random_uniform([51764 + 2, embedding_id], -0.1, 0.1), name=\"iidmf\")\n",
    "            self.iid = tf.nn.embedding_lookup(iidmf,self.input_iid)   # item features   (p_i)\n",
    "            self.iid = tf.reshape(self.iid,[-1,embedding_id])\n",
    "            \n",
    "            Wi = tf.Variable( tf.random_uniform([rnn_size, n_latent], -0.1, 0.1), name='Wi')\n",
    "            bi = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bi\")\n",
    "            self.i_feas = tf.matmul(self.i_feas, Wi)+self.iid + bi    # Yi+pi\n",
    "            print('Yi+pi',self.i_feas.shape)\n",
    "            self.i_feas = tf.identity(self.i_feas, name=\"feature_item\")\n",
    "######################################################################            \n",
    "        with tf.name_scope(\"FM\"):\n",
    "            self.FM = tf.multiply(self.u_feas, self.i_feas)\n",
    "            self.FM = tf.nn.relu(self.FM)\n",
    "            print('FM ',self.FM.shape)\n",
    "            self.FM=tf.nn.dropout(self.FM,rate=1-self.dropout_keep_prob)\n",
    "# ######################################################################            \n",
    "            Wmul=tf.Variable(tf.random_uniform([n_latent, 1], -0.1, 0.1), name='wmul')\n",
    "            self.mul=tf.matmul(self.FM,Wmul)\n",
    "            self.score=tf.reduce_sum(self.mul,1,keepdims=True,name=\"score\")\n",
    "            print('score: ',self.score.shape)\n",
    "\n",
    "#######################################################################\n",
    "        with tf.name_scope(\"biases\"):\n",
    "            self.uidW = tf.Variable(tf.constant(0.1, shape=[51764 + 2]), name=\"uidW\")\n",
    "            self.iidW = tf.Variable(tf.constant(0.1, shape=[51764 + 2]), name=\"iidW\")\n",
    "            self.u_bias = tf.gather(self.uidW, self.input_uid)\n",
    "            print('self.u_bias',self.u_bias.shape)\n",
    "            self.i_bias = tf.gather(self.iidW, self.input_iid)\n",
    "            self.Feature_bias = self.u_bias + self.i_bias\n",
    "            self.biased = tf.Variable(tf.constant(0.1), name='bias')\n",
    "#######################################################################            \n",
    "        with tf.name_scope(\"prediction\"):\n",
    "            self.predictions = self.score + self.Feature_bias + self.biased    # rate nahayi\n",
    "            self.predictions = tf.identity(self.predictions, name=\"predictions\")\n",
    "            print('predictions',self.predictions.shape)\n",
    "#######################################################################            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.l2_loss(tf.subtract(self.predictions, self.input_y))\n",
    "            self.loss = losses + l2_reg_lambda * l2_loss  #amalan l2_loss=0\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.MAE = tf.reduce_mean(tf.abs(tf.subtract(self.predictions, self.input_y)),name=\"MAE\")\n",
    "            self.RMSE =tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.predictions, self.input_y))),name='RMSE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 991911,
     "status": "ok",
     "timestamp": 1596205964137,
     "user": {
      "displayName": "narges farrokhshad",
      "photoUrl": "",
      "userId": "09411964733981171008"
     },
     "user_tz": -270
    },
    "id": "2NVddFUT27_N",
    "outputId": "13cbaf66-7212-435e-ca89-15703c99f2d0"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime \n",
    "import pytz \n",
    "  \n",
    "IST = pytz.timezone('Asia/Tehran') \n",
    "datetime_ist = datetime.now(IST) \n",
    "current_time = datetime_ist.strftime('%H%M%S')\n",
    "\n",
    "import time\n",
    "tf.reset_default_graph()   # To clear the defined variables and operations of the previous cell\n",
    "\n",
    "# with tf.Graph().as_default():\n",
    "session_conf = tf.ConfigProto(allow_soft_placement=allow_soft_placement, log_device_placement=log_device_placement)\n",
    "session_conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=session_conf)\n",
    "with sess.as_default():\n",
    "    deep = NARRE( review_num_u=review_num_u, review_num_i=review_num_i,  review_len_u=review_len_u,  review_len_i=review_len_i,\n",
    "        user_num=user_num, item_num=item_num, num_classes=1, user_vocab_size=len(vocabulary_user), item_vocab_size=len(vocabulary_item),\n",
    "        embedding_size=embedding_dim,  embedding_id=32,l2_reg_lambda=l2_reg_lambda, attention_size=32,  n_latent=32)\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    # optimizer = tf.train.AdagradOptimizer(learning_rate=0.01, initial_accumulator_value=1e-8).minimize(deep.loss)\n",
    "    optimizer =tf.train.AdamOptimizer(0.005) # tf.train.AdamOptimizer(0.002, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(deep.loss)\n",
    "    grads_and_vars = optimizer.compute_gradients(deep.loss)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "    grad_summaries = []\n",
    "    for g, v in grads_and_vars:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "    # out_dir = os.path.join('/content/drive/My Drive/dataset/chekpoint_music/run', current_time)\n",
    "    out_dir='run/'+current_time\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", deep.loss)\n",
    "    acc_summary = tf.summary.scalar(\"RMSE\", deep.RMSE)\n",
    "    MAE_summary = tf.summary.scalar(\"MAE\", deep.MAE)\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, acc_summary,MAE_summary,grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Dev summaries\n",
    "    dev_summary_op = tf.summary.merge([loss_summary, acc_summary,MAE_summary])\n",
    "    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()     \n",
    "\n",
    "    # load any vectors from the word2vec         \n",
    "    sess.run(deep.W1.assign(initW1))   \n",
    "    sess.run(deep.W2.assign(initW2))\n",
    "\n",
    "    epoch = 15\n",
    "    best_mae =5\n",
    "    best_rmse =5\n",
    "    train_mae = 0\n",
    "    train_rmse = 0       \n",
    "    batch_size=100\n",
    "\n",
    "    total_batch_train = int(train_length / batch_size) #40\n",
    "    total_batch_test = int(test_length / batch_size)  #5  \n",
    "    print(\"total_batch_num: \",total_batch_train,\"\\n\")\n",
    "    for epoch in range(epoch):\n",
    "        # Shuffle the data at each epoch\n",
    "        shuffle_indices = np.random.permutation(np.arange(train_length))\n",
    "        shuffled_data = train_data[shuffle_indices]\n",
    "        datetime_ist = datetime.now(IST) \n",
    "        time_str= datetime_ist.strftime('%H:%M:%S')\n",
    "        print(time_str,\" epoch_\"+str(epoch),\"started... \")            \n",
    "                \n",
    "        for batch_num in range(total_batch_train):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, train_length)\n",
    "            data_train = shuffled_data[start_index:end_index]\n",
    "            uid, iid, y_batch = zip(*data_train) # reuid hamun reid_user\n",
    "            u_batch = []\n",
    "            i_batch = []\n",
    "            for i in range(len(uid)):\n",
    "                u_batch.append(u_text[uid[i][0]])\n",
    "                i_batch.append(i_text[iid[i][0]])\n",
    "            u_batch = np.array(u_batch)\n",
    "            i_batch = np.array(i_batch)\n",
    "            t_rmse, t_mae, fm,Train_prediction,y = train_step(u_batch, i_batch, uid, iid, y_batch,batch_num)\n",
    "           \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            train_rmse += t_rmse\n",
    "            train_mae += t_mae\n",
    "\n",
    "            if batch_num%10 == 0 and batch_num > 1:                                                     \n",
    "                loss_s = 0\n",
    "                accuracy_s = 0\n",
    "                mae_s = 0\n",
    "\n",
    "                for batch_num in range(total_batch_test):\n",
    "                    start_index = batch_num * batch_size\n",
    "                    end_index = min((batch_num + 1) * batch_size, test_length)\n",
    "                    data_test = test_data[start_index:end_index]\n",
    "\n",
    "                    userid_valid, itemid_valid, y_valid = zip(*data_test)\n",
    "                    u_valid = []\n",
    "                    i_valid = []\n",
    "                    for i in range(len(userid_valid)):\n",
    "                        u_valid.append(u_text[userid_valid[i][0]])\n",
    "                        i_valid.append(i_text[itemid_valid[i][0]])\n",
    "                    u_valid = np.array(u_valid)\n",
    "                    i_valid = np.array(i_valid)\n",
    "\n",
    "                    loss, accuracy, mae,prediction,y = dev_step(u_valid, i_valid, userid_valid, itemid_valid, y_valid,writer=dev_summary_writer)\n",
    "                    loss_s = loss_s + len(u_valid) * loss\n",
    "                    accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "                    mae_s = mae_s + len(u_valid) * mae\n",
    "\n",
    "                rmse = np.sqrt(accuracy_s/test_length)  #baraye hameye valid ha \n",
    "                mae = mae_s / test_length\n",
    "                if best_rmse > rmse:              #age koochiktar peyda kardi jaygozin kon\n",
    "                    best_rmse = rmse\n",
    "                if best_mae > mae:\n",
    "                    best_mae = mae\n",
    "        datetime_ist = datetime.now(IST) \n",
    "        time_str= datetime_ist.strftime('%H:%M:%S')    \n",
    "        print (time_str,\" epoch_\"+str(epoch)+\" ended.\\n\" +\"Evaluation:\")\n",
    "\n",
    "        print (\"train:rmse= {:g}, mae= {:g}\".format(train_rmse/total_batch_train , train_mae/total_batch_train))   #bad az hameye batches     \n",
    "        train_rmse =0\n",
    "        train_mae = 0\n",
    "        loss_s = 0\n",
    "        accuracy_s = 0\n",
    "        mae_s = 10\n",
    "\n",
    "        for batch_num in range(total_batch_test):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, test_length)\n",
    "            data_test = test_data[start_index:end_index]\n",
    "\n",
    "            userid_valid, itemid_valid, y_valid = zip(*data_test)\n",
    "            u_valid = []\n",
    "            i_valid = []\n",
    "            for i in range(len(userid_valid)):\n",
    "                u_valid.append(u_text[userid_valid[i][0]])\n",
    "                i_valid.append(i_text[itemid_valid[i][0]])\n",
    "            u_valid = np.array(u_valid)\n",
    "            i_valid = np.array(i_valid)\n",
    "\n",
    "            loss, accuracy, mae,prediction,y = dev_step(u_valid, i_valid, userid_valid, itemid_valid, y_valid,writer=dev_summary_writer)\n",
    "\n",
    "            loss_s = loss_s + len(u_valid) * loss\n",
    "            accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "            mae_s = mae_s + len(u_valid) * mae\n",
    "        print (\"valid:rmse= {:g}, mae= {:g}\".format(np.sqrt(accuracy_s / test_length), mae_s / test_length))\n",
    "        \n",
    "        #                 path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "#                 print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "        rmse = np.sqrt(accuracy_s / test_length)\n",
    "        mae = mae_s / test_length\n",
    "        if best_rmse > rmse:\n",
    "            best_rmse = rmse\n",
    "        if best_mae > mae:\n",
    "            best_mae = mae\n",
    "\n",
    "        if ((epoch+1)%10==0 and epoch>1):\n",
    "            saved_path = saver.save(sess, '/content/drive/My Drive/dataset/chekpoint_music/new/music', global_step=global_step)\n",
    "            print('model saved in {}'.format(saved_path))\n",
    "       \n",
    "    print ('best rmse:', best_rmse)\n",
    "    print ('best mae:', best_mae)\n",
    "\n",
    "#163939 moheme\n",
    "#104515 moheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1593201349449,
     "user": {
      "displayName": "narges farrokhshad",
      "photoUrl": "",
      "userId": "09411964733981171008"
     },
     "user_tz": -270
    },
    "id": "dFkz_CmvMUxl",
    "outputId": "5aef2959-2725-490c-8a18-dde2e609bf8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved in /content/drive/My Drive/dataset/chekpoint_music/new/musicLast\n"
     ]
    }
   ],
   "source": [
    "saved_path = saver.save(sess, '/content/drive/My Drive/dataset/chekpoint_music/new/musicLast')\n",
    "print('model saved in {}'.format(saved_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1r896npQ4cH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,precision_recall_curve\n",
    "\n",
    "precision= precision_score(y,prediction.astype(int),average='weighted')\n",
    "recall=recall_score(y,prediction.astype(int),average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZpBM3unByeX"
   },
   "outputs": [],
   "source": [
    "!kill 366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swE95HDG4bVR"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir='/content/drive/My Drive/dataset/chekpoint_music/run/104515/summaries/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ws8utZG2OG5B"
   },
   "outputs": [],
   "source": [
    "vars_list = tf.train.list_variables('/content/drive/My Drive/dataset/chekpoint_music/new') #just variables \n",
    "vars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18532,
     "status": "ok",
     "timestamp": 1595586498828,
     "user": {
      "displayName": "narges farrokhshad",
      "photoUrl": "",
      "userId": "09411964733981171008"
     },
     "user_tz": -270
    },
    "id": "pKBwaYBW6q3g",
    "outputId": "9348a6aa-e387-40ac-adb6-03bf4a517e9c"
   },
   "outputs": [],
   "source": [
    "#test model\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "pkl_file = open('/content/drive/My Drive/dataset/music/title/music.train', 'rb')\n",
    "train_data = pickle.load(pkl_file)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    saver = tf.train.import_meta_graph('/content/drive/My Drive/dataset/chekpoint_music/new/music-6880.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('/content/drive/My Drive/dataset/chekpoint_music/new'))\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    input_u = graph.get_tensor_by_name('input_u:0')\n",
    "    input_i = graph.get_tensor_by_name('input_i:0')\n",
    "    input_y = graph.get_tensor_by_name('input_y:0')\n",
    "    input_uid = graph.get_tensor_by_name('input_uid:0')\n",
    "    input_iid = graph.get_tensor_by_name('input_iid:0')\n",
    "    prediction = graph.get_tensor_by_name(\"prediction/predictions:0\")\n",
    "    score= graph.get_tensor_by_name(\"attention_user/attention_score_user:0\")\n",
    "    \n",
    "    data_train = train_data[:100]\n",
    "    uid, iid, y_batch = zip(*data_train) # reuid hamun reid_user\n",
    "    u_batch = []\n",
    "    i_batch = []\n",
    "    for i in range(len(uid)):\n",
    "        u_batch.append(u_text[uid[i][0]])\n",
    "        i_batch.append(i_text[iid[i][0]])\n",
    "    u_batch = np.array(u_batch)\n",
    "    i_batch = np.array(i_batch)\n",
    "\n",
    "    feed_dict = {                         #placeholder haro meghdardehi sabet mikone\n",
    "        input_u: u_batch,\n",
    "        input_i: i_batch,\n",
    "        input_uid: uid,\n",
    "        input_iid: iid,\n",
    "        input_y: y_batch,\n",
    "    }\n",
    "    prediction = sess.run(prediction,feed_dict)\n",
    "    attention_score=sess.run(score,feed_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEaOHyAK2NYI"
   },
   "outputs": [],
   "source": [
    "print(uid[0][0])\n",
    "print(iid[0][0])\n",
    "print(y_batch[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hE7UVTSsKpHB"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/content/drive/My Drive/dataset/music/title/music_processed.csv\")\n",
    "# df.loc[df['user_id']==2426]\n",
    "df[\"reviews\"][17839]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 731,
     "status": "ok",
     "timestamp": 1595526395483,
     "user": {
      "displayName": "narges farrokhshad",
      "photoUrl": "",
      "userId": "09411964733981171008"
     },
     "user_tz": -270
    },
    "id": "Hg2zJuLxNc9L",
    "outputId": "c1c1bfb1-b696-4e3b-dde3-7f5a6465bf69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of wrong answers\n",
    "np.sum(np.array(prediction).round() != y_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2147,
     "status": "ok",
     "timestamp": 1593263661750,
     "user": {
      "displayName": "narges farrokhshad",
      "photoUrl": "",
      "userId": "09411964733981171008"
     },
     "user_tz": -270
    },
    "id": "yNmvwxBqRYy6",
    "outputId": "4d763163-4dbc-4573-9207-f307edb5286c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.3418446] [5.319431] [3.9605584] [4.451573] [5.1865077]\n",
      "[5.] [5.] [5.] [4.] [5.]\n"
     ]
    }
   ],
   "source": [
    "print(*prediction[:5])\n",
    "print(*y_batch[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2047,
     "status": "ok",
     "timestamp": 1593264750837,
     "user": {
      "displayName": "narges farrokhshad",
      "photoUrl": "",
      "userId": "09411964733981171008"
     },
     "user_tz": -270
    },
    "id": "z201EXPZYABC",
    "outputId": "1b87e90f-cfb5-4ce0-8f63-859576ada8c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563\n"
     ]
    }
   ],
   "source": [
    "#recommendation for a user\n",
    "uid, iid, y_batch = zip(*train_data[:100])\n",
    "for i in range(100):\n",
    "  if(uid[i][0]==2426):\n",
    "    if(prediction[i]>4.0):\n",
    "      print(iid[i][0] )\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOmgbG6ASJaigYKBm7FxXvJ",
   "collapsed_sections": [],
   "name": "bestVersin_RNN.ipynb",
   "provenance": [
    {
     "file_id": "1ZE6Lzg66GZlKQ28we-XkJc0ro0ADU2KQ",
     "timestamp": 1591615987943
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
