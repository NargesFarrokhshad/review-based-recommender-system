{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bestVersin_RNN.ipynb","provenance":[{"file_id":"1ZE6Lzg66GZlKQ28we-XkJc0ro0ADU2KQ","timestamp":1591615987943}],"collapsed_sections":[],"authorship_tag":"ABX9TyOmgbG6ASJaigYKBm7FxXvJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HcsV3XIp3dKY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1596195399442,"user_tz":-270,"elapsed":26345,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"2ebd4717-aeac-4407-8a33-858f35fe78b6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aalWwhfh21f4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596195401461,"user_tz":-270,"elapsed":2011,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}}},"source":["def train_step(u_batch, i_batch, uid, iid, y_batch,batch_num):\n","    \"\"\"\n","    A single training step\n","    \"\"\"\n","    feed_dict = {                         #placeholder haro meghdardehi sabet mikone\n","        deep.input_u: u_batch,\n","        deep.input_i: i_batch,\n","        deep.input_uid: uid,\n","        deep.input_iid: iid,\n","        deep.input_y: y_batch,\n","    }\n","    _, step,summaries, loss, RMSE, MAE, fm,prediction,y = sess.run(\n","        [train_op, global_step,train_summary_op, deep.loss, deep.RMSE, deep.MAE,deep.score,deep.predictions,deep.input_y]\n","        , feed_dict)\n","     \n","    # time_str = datetime.datetime.now().strftime(\"%X\")\n","    # print(\"{}: step {}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, batch_num, loss, RMSE, MAE))\n","    train_summary_writer.add_summary(summaries, step)\n","    return RMSE, MAE,fm,prediction,y\n","  \n","def dev_step(u_batch, i_batch, uid, iid, y_batch,writer=None):\n","    \"\"\"\n","    Evaluates model on a dev set\n","\n","    \"\"\"\n","    feed_dict = {\n","        deep.input_u: u_batch,\n","        deep.input_i: i_batch,\n","        deep.input_y: y_batch,\n","        deep.input_uid: uid,\n","        deep.input_iid: iid,\n","    }\n","    step,summaries, loss, RMSE, mae,prediction,y = sess.run(\n","        [global_step,dev_summary_op, deep.loss, deep.RMSE, deep.MAE,deep.predictions,deep.input_y], feed_dict)\n","    # time_str = datetime.datetime.now().strftime(\"%X\")\n","#     print(\"{}: step{}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, step, loss, accuracy, mae))\n","    if writer:\n","        writer.add_summary(summaries, step)\n","    return [loss, RMSE, mae,prediction,y]  "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"aI1GXlJe246A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"executionInfo":{"status":"ok","timestamp":1596195406699,"user_tz":-270,"elapsed":7240,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"a2acf273-ab90-40a7-e955-393dc44718b6"},"source":["import numpy as np\n","import tensorflow as tf\n","import pickle\n","import datetime\n","import sys\n","import time\n","import os\n","\n","\n","word2vec=\"/content/drive/My Drive/dataset/glove.6B.100d.txt\"\n","valid_data=\"/content/drive/My Drive/dataset/music/project/music.test\"\n","para_data= \"/content/drive/My Drive/dataset/music/project/music.para\"\n","train_data= \"/content/drive/My Drive/dataset/music/project/music.train\"\n","# ==================================================\n","\n","# Model Hyperparameters\n","embedding_dim= 100\n","l2_reg_lambda= 0.001\n","\n","# Misc Parameters\n","allow_soft_placement= True\n","log_device_placement= False\n","\n","print(\"Loading data...\")\n","\n","# with open (para_data, 'rb') as pkl_file:\n","#   pkl_file.close()\n","pkl_file = open(para_data, 'rb')\n","\n","  \n","para = pickle.load(pkl_file)\n","user_num = para['user_num']\n","item_num = para['item_num']\n","review_num_u = para['review_num_u']\n","review_num_i = para['review_num_i']\n","review_len_u = para['review_len_u']\n","review_len_i = para['review_len_i']\n","vocabulary_user = para['user_vocab']\n","vocabulary_item = para['item_vocab']\n","train_length = para['train_length']\n","test_length = para['test_length']\n","u_text = para['u_text']\n","i_text = para['i_text']\n","\n","np.random.seed(2017)\n","random_seed = 2017\n","print (user_num)\n","print (item_num)\n","print (review_num_u)\n","print (review_len_u)\n","print (review_num_i)\n","print (review_len_i)\n","\n","pkl_file = open(train_data, 'rb')\n","\n","train_data = pickle.load(pkl_file)\n","\n","train_data = np.array(train_data)\n","pkl_file.close()\n","\n","pkl_file = open(valid_data, 'rb')\n","\n","test_data = pickle.load(pkl_file)\n","test_data = np.array(test_data)\n","pkl_file.close()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Loading data...\n","4992\n","3328\n","9\n","294\n","16\n","295\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6zoU8CLX28AA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1596195416077,"user_tz":-270,"elapsed":16608,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"d928b3ec-4ffe-4ffc-8a3d-98b6cf373452"},"source":["u=0\n","v=0\n","initW1 = np.random.uniform(-1.0, 1.0, (len(vocabulary_user), embedding_dim))\n","\n","with open(word2vec,encoding=\"utf-8\") as f:\n","    for line in f:\n","        p=line.split()\n","        word=p[0]\n","        if word in vocabulary_user:\n","            u = u + 1\n","            idx = vocabulary_user[word]\n","            initW1[idx] = np.asarray(p[1:], dtype=\"float32\")\n","initW2 = np.random.uniform(-1.0, 1.0, (len(vocabulary_item),embedding_dim))\n","\n","with open(word2vec,encoding=\"utf-8\") as f:\n","    for line in f:\n","        p=line.split()\n","        word=p[0]\n","        if word in vocabulary_item:\n","            v=v+1\n","            idx = vocabulary_item[word]\n","            initW2[idx] = np.asarray(p[1:], dtype=\"float32\")\n","                        \n","print (\"in vocab_u \", u)\n","print (\"in vocab_i \", v)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["in vocab_u  9545\n","in vocab_i  10189\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rzIIAROC3AOl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596195416078,"user_tz":-270,"elapsed":16600,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}}},"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","\n","import tensorflow as tff\n","class NARRE(object):\n","    def __init__(\n","            self, review_num_u, review_num_i, review_len_u, review_len_i, user_num, item_num, num_classes,\n","            user_vocab_size, item_vocab_size, n_latent, embedding_id, attention_size,\n","            embedding_size, l2_reg_lambda=0.1):\n","        self.input_u = tf.placeholder(tf.int32, [None, review_num_u, review_len_u], name=\"input_u\")\n","        self.input_i = tf.placeholder(tf.int32, [None, review_num_i, review_len_i], name=\"input_i\")\n","        self.input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n","        self.input_uid = tf.placeholder(tf.int32, [None, 1], name=\"input_uid\")\n","        self.input_iid = tf.placeholder(tf.int32, [None, 1], name=\"input_iid\")\n","\n","        print('input user shape:',self.input_u.shape)\n","        l2_loss = tf.constant(0.0)\n","        self.dropout_keep_prob=tf.constant(0.7)\n","    #############################################\n","        with tf.name_scope(\"user_embedding\"):\n","            self.W1 = tf.Variable(tf.random_uniform([user_vocab_size, embedding_size], -1.0, 1.0),name=\"W1\",trainable=False)\n","            self.embedded_user = tf.nn.embedding_lookup(self.W1, self.input_u)                            #(100,16,441,50)\n","            print('self.embedded_users.shape',self.embedded_user.shape)\n","\n","        with tf.name_scope(\"item_embedding\"):\n","            self.W2 = tf.Variable(tf.random_uniform([item_vocab_size, embedding_size], -1.0, 1.0),name=\"W2\",trainable=False)\n","            self.embedded_item = tf.nn.embedding_lookup(self.W2, self.input_i)\n","            print('self.embedded_item.shape',self.embedded_item.shape)\n","   #######################################################         \n","        with tf.name_scope(\"LSTM_cell\"):\n","            rnn_size=100\n","            num_layers=1\n","        with tf.variable_scope(\"scope\", reuse = False ): \n","            \n","            rnn_cell1 = tf.compat.v1.nn.rnn_cell.LSTMCell(rnn_size,name=\"rnn_user\",reuse = tf.AUTO_REUSE)\n","        with tf.variable_scope(\"scope2\",reuse = False): \n","            rnn_cell2 = tf.compat.v1.nn.rnn_cell.LSTMCell(rnn_size,name=\"rnn_item\",reuse = tf.AUTO_REUSE)\n","\n","            \n","        with tf.name_scope(\"LSTM_User\"):\n","            user_jadid=tf.reshape(self.embedded_user,[-1, review_len_u, embedding_size])\n","            print('user jadid shape: ',user_jadid.shape)\n","            self.rnn_outputs_user, user_state = tf.nn.dynamic_rnn(\n","                cell=rnn_cell1, inputs=user_jadid , dtype=user_jadid.dtype)\n","            \n","            self.rnn_outputs_user = tf.nn.dropout(self.rnn_outputs_user, rate=1-self.dropout_keep_prob)   # khorooji CNN user _ feature vector for  every user  - O_ul   (100,16,100)\n","            self.h_pool_flat_u = tf.reshape(self.rnn_outputs_user, [-1, review_num_u*review_len_u, rnn_size])  \n","\n","        with tf.name_scope(\"LSTM_item\"):\n","            item_jadid=tf.reshape(self.embedded_item,[-1, review_len_i, embedding_size])\n","            print('item jadid shape: ',item_jadid.shape)\n","            self.rnn_outputs_item, item_state = tf.nn.dynamic_rnn(\n","                cell=rnn_cell2, inputs=item_jadid ,dtype=item_jadid.dtype)\n","            self.rnn_outputs_item = tf.nn.dropout(self.rnn_outputs_item, rate=1-self.dropout_keep_prob)   #khorooji CNN item _ feature vector for every item        with tf.name_scope(\"dropout_item\"):    \n","            self.h_pool_flat_i = tf.reshape(self.rnn_outputs_item, [-1, review_num_i*review_len_i, rnn_size])  \n","\n","            # stacked = tf.nn.rnn_cell.MultiRNNCell(fw_cells, state_is_tuple=True)\n","            # self.rnn_outputs_user, _ = tf.nn.dynamic_rnn(\n","            #     cell=stacked, inputs=user_jadid , dtype=self.embedded_user.dtype)\n","            # print('a')\n","            # self.rnn_outputs_item, _ = tf.nn.dynamic_rnn(\n","            #     cell=stacked, inputs=item_jadid ,dtype=self.embedded_item.dtype)\n","\n","            print('rnn_outputs_user shape',(self.rnn_outputs_user[0].shape,self.rnn_outputs_user[1].shape))\n","            print('rnn_outputs_item shape',(self.rnn_outputs_item[0].shape,self.rnn_outputs_item[1].shape))\n","#######################################################################\n","\n","        # with tf.name_scope(\"dropout_user\"):\n","\n","        print('user_dropOut',self.rnn_outputs_user.shape) \n","        print('item_dropOut',self.rnn_outputs_item.shape)\n","\n","        print('flat output user:',self.h_pool_flat_u.shape )  \n","        print('flat output item:',self.h_pool_flat_i.shape )\n","###################################### ##########################           \n","        with tf.name_scope(\"attention_user\"): \n","            Wau = tf.Variable(tf.random_uniform([rnn_size, attention_size], -0.1, 0.1), name='Wau')  #Wo\n","            Wpu = tf.Variable(tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpu')  #hT\n","            bau = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bau\")    #b1\n","            bbu = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbu\")    #b2\n","            self.u_j=tf.matmul(tf.nn.relu(tf.matmul(self.h_pool_flat_u, Wau)+ bau),Wpu)+bbu  # None*u_len*1   a*_ul  (100,16,1)\n","\n","            self.attention_score_user = tf.nn.softmax(self.u_j,1,name=\"attention_score_user\")  # none*u_len*1    vazne atention user   a_ul  (100,16,1)\n","            print('attention_score_user',self.attention_score_user.shape)\n","           \n","        with tf.name_scope(\"attention_item\"):  \n","\n","            Wai = tf.Variable( tf.random_uniform([rnn_size, attention_size], -0.1, 0.1), name='Wai')\n","            Wpi = tf.Variable( tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpi')\n","            bai = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bai\")\n","            bbi = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbi\")\n","           \n","            self.i_j=tf.matmul(tf.nn.relu(tf.matmul(self.h_pool_flat_i, Wai)+ bai),Wpi)+bbi\n","\n","            self.attention_score_item = tf.nn.softmax(self.i_j,1,name=\"attention_score_item\")  # none*len*1     # vazne attention item   contribution of the lth review to the feature profile of item i:\n","            print('attention_score_item',self.attention_score_item.shape)\n","\n","###################################################################\n","\n","        with tf.name_scope(\"rated_review_user\"):\n","            self.u_feast = tf.reduce_sum(tf.multiply(self.attention_score_user, self.h_pool_flat_u), 1)\n","            self.u_feas = tf.nn.dropout(self.u_feast, rate=1-self.dropout_keep_prob,name=\"rated_review_user\")  # feature vector for user - user representation (Xu)\n","            \n","            print('user_feature vector:',self.u_feas.shape)\n","        with tf.name_scope(\"rated_review_item\"):\n","            self.i_feas = tf.reduce_sum(tf.multiply(self.attention_score_item, self.h_pool_flat_i), 1)\n","            self.i_feas = tf.nn.dropout(self.i_feas, rate=1-self.dropout_keep_prob,name=\"rated_review_item\")  # feature vector for item - item representation (Yi)\n","            print('item_feature vector:',self.i_feas.shape)\n"," ###################################################################           \n","        with tf.name_scope(\"feature_user\"):\n","            uidmf = tf.Variable(tf.random_uniform([51764 + 2, embedding_id], -0.1, 0.1), name=\"uidmf\")\n","\n","            self.uid = tf.nn.embedding_lookup(uidmf,self.input_uid)   # user prefrences (q_u)  \n","            self.uid = tf.reshape(self.uid,[-1,embedding_id])\n","            Wu = tf.Variable( tf.random_uniform([rnn_size, n_latent], -0.1, 0.1), name='Wu')  #100*32\n","            bu = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bu\")\n","            self.u_feas = tf.matmul(self.u_feas, Wu)+self.uid + bu    #Xu+qu\n","            self.u_feas = tf.identity(self.u_feas, name=\"feature_user\")\n","\n","            print('Xu+qu',self.u_feas.shape)\n","        with tf.name_scope(\"feature_item\"):\n","            iidmf = tf.Variable(tf.random_uniform([51764 + 2, embedding_id], -0.1, 0.1), name=\"iidmf\")\n","            self.iid = tf.nn.embedding_lookup(iidmf,self.input_iid)   # item features   (p_i)\n","            self.iid = tf.reshape(self.iid,[-1,embedding_id])\n","            \n","            Wi = tf.Variable( tf.random_uniform([rnn_size, n_latent], -0.1, 0.1), name='Wi')\n","            bi = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bi\")\n","            self.i_feas = tf.matmul(self.i_feas, Wi)+self.iid + bi    # Yi+pi\n","            print('Yi+pi',self.i_feas.shape)\n","            self.i_feas = tf.identity(self.i_feas, name=\"feature_item\")\n","######################################################################            \n","        with tf.name_scope(\"FM\"):\n","            self.FM = tf.multiply(self.u_feas, self.i_feas)\n","            self.FM = tf.nn.relu(self.FM)\n","            print('FM ',self.FM.shape)\n","            self.FM=tf.nn.dropout(self.FM,rate=1-self.dropout_keep_prob)\n","# ######################################################################            \n","            Wmul=tf.Variable(tf.random_uniform([n_latent, 1], -0.1, 0.1), name='wmul')\n","            self.mul=tf.matmul(self.FM,Wmul)\n","            self.score=tf.reduce_sum(self.mul,1,keepdims=True,name=\"score\")\n","            print('score: ',self.score.shape)\n","\n","#######################################################################\n","        with tf.name_scope(\"biases\"):\n","            self.uidW = tf.Variable(tf.constant(0.1, shape=[51764 + 2]), name=\"uidW\")\n","            self.iidW = tf.Variable(tf.constant(0.1, shape=[51764 + 2]), name=\"iidW\")\n","            self.u_bias = tf.gather(self.uidW, self.input_uid)\n","            print('self.u_bias',self.u_bias.shape)\n","            self.i_bias = tf.gather(self.iidW, self.input_iid)\n","            self.Feature_bias = self.u_bias + self.i_bias\n","            self.biased = tf.Variable(tf.constant(0.1), name='bias')\n","#######################################################################            \n","        with tf.name_scope(\"prediction\"):\n","            self.predictions = self.score + self.Feature_bias + self.biased    # rate nahayi\n","            self.predictions = tf.identity(self.predictions, name=\"predictions\")\n","            print('predictions',self.predictions.shape)\n","#######################################################################            \n","        with tf.name_scope(\"loss\"):\n","            losses = tf.nn.l2_loss(tf.subtract(self.predictions, self.input_y))\n","            self.loss = losses + l2_reg_lambda * l2_loss  #amalan l2_loss=0\n","\n","        with tf.name_scope(\"accuracy\"):\n","            self.MAE = tf.reduce_mean(tf.abs(tf.subtract(self.predictions, self.input_y)),name=\"MAE\")\n","            self.RMSE =tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.predictions, self.input_y))),name='RMSE')\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"2NVddFUT27_N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596205964137,"user_tz":-270,"elapsed":991911,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"13cbaf66-7212-435e-ca89-15703c99f2d0"},"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","tf.logging.set_verbosity(tf.logging.WARN)\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from datetime import datetime \n","import pytz \n","  \n","IST = pytz.timezone('Asia/Tehran') \n","datetime_ist = datetime.now(IST) \n","current_time = datetime_ist.strftime('%H%M%S')\n","\n","import time\n","tf.reset_default_graph()   # To clear the defined variables and operations of the previous cell\n","\n","# with tf.Graph().as_default():\n","session_conf = tf.ConfigProto(allow_soft_placement=allow_soft_placement, log_device_placement=log_device_placement)\n","session_conf.gpu_options.allow_growth = True\n","sess = tf.Session(config=session_conf)\n","with sess.as_default():\n","    deep = NARRE( review_num_u=review_num_u, review_num_i=review_num_i,  review_len_u=review_len_u,  review_len_i=review_len_i,\n","        user_num=user_num, item_num=item_num, num_classes=1, user_vocab_size=len(vocabulary_user), item_vocab_size=len(vocabulary_item),\n","        embedding_size=embedding_dim,  embedding_id=32,l2_reg_lambda=l2_reg_lambda, attention_size=32,  n_latent=32)\n","    tf.set_random_seed(random_seed)\n","    \n","    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n","    # optimizer = tf.train.AdagradOptimizer(learning_rate=0.01, initial_accumulator_value=1e-8).minimize(deep.loss)\n","    optimizer =tf.train.AdamOptimizer(0.005) # tf.train.AdamOptimizer(0.002, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(deep.loss)\n","    grads_and_vars = optimizer.compute_gradients(deep.loss)\n","    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n","\n","    grad_summaries = []\n","    for g, v in grads_and_vars:\n","        if g is not None:\n","            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n","            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n","            grad_summaries.append(grad_hist_summary)\n","            grad_summaries.append(sparsity_summary)\n","    grad_summaries_merged = tf.summary.merge(grad_summaries)\n","\n","    # out_dir = os.path.join('/content/drive/My Drive/dataset/chekpoint_music/run', current_time)\n","    out_dir='run/'+current_time\n","    # Summaries for loss and accuracy\n","    loss_summary = tf.summary.scalar(\"loss\", deep.loss)\n","    acc_summary = tf.summary.scalar(\"RMSE\", deep.RMSE)\n","    MAE_summary = tf.summary.scalar(\"MAE\", deep.MAE)\n","    # Train Summaries\n","    train_summary_op = tf.summary.merge([loss_summary, acc_summary,MAE_summary,grad_summaries_merged])\n","    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n","    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n","\n","    # Dev summaries\n","    dev_summary_op = tf.summary.merge([loss_summary, acc_summary,MAE_summary])\n","    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n","    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n","\n","    sess.run(tf.global_variables_initializer())\n","    saver = tf.train.Saver()     \n","\n","    # load any vectors from the word2vec         \n","    sess.run(deep.W1.assign(initW1))   \n","    sess.run(deep.W2.assign(initW2))\n","\n","    epoch = 15\n","    best_mae =5\n","    best_rmse =5\n","    train_mae = 0\n","    train_rmse = 0       \n","    batch_size=100\n","\n","    total_batch_train = int(train_length / batch_size) #40\n","    total_batch_test = int(test_length / batch_size)  #5  \n","    print(\"total_batch_num: \",total_batch_train,\"\\n\")\n","    for epoch in range(epoch):\n","        # Shuffle the data at each epoch\n","        shuffle_indices = np.random.permutation(np.arange(train_length))\n","        shuffled_data = train_data[shuffle_indices]\n","        datetime_ist = datetime.now(IST) \n","        time_str= datetime_ist.strftime('%H:%M:%S')\n","        print(time_str,\" epoch_\"+str(epoch),\"started... \")            \n","                \n","        for batch_num in range(total_batch_train):\n","            start_index = batch_num * batch_size\n","            end_index = min((batch_num + 1) * batch_size, train_length)\n","            data_train = shuffled_data[start_index:end_index]\n","            uid, iid, y_batch = zip(*data_train) # reuid hamun reid_user\n","            u_batch = []\n","            i_batch = []\n","            for i in range(len(uid)):\n","                u_batch.append(u_text[uid[i][0]])\n","                i_batch.append(i_text[iid[i][0]])\n","            u_batch = np.array(u_batch)\n","            i_batch = np.array(i_batch)\n","            t_rmse, t_mae, fm,Train_prediction,y = train_step(u_batch, i_batch, uid, iid, y_batch,batch_num)\n","           \n","            current_step = tf.train.global_step(sess, global_step)\n","            train_rmse += t_rmse\n","            train_mae += t_mae\n","\n","            if batch_num%10 == 0 and batch_num > 1:                                                     \n","                loss_s = 0\n","                accuracy_s = 0\n","                mae_s = 0\n","\n","                for batch_num in range(total_batch_test):\n","                    start_index = batch_num * batch_size\n","                    end_index = min((batch_num + 1) * batch_size, test_length)\n","                    data_test = test_data[start_index:end_index]\n","\n","                    userid_valid, itemid_valid, y_valid = zip(*data_test)\n","                    u_valid = []\n","                    i_valid = []\n","                    for i in range(len(userid_valid)):\n","                        u_valid.append(u_text[userid_valid[i][0]])\n","                        i_valid.append(i_text[itemid_valid[i][0]])\n","                    u_valid = np.array(u_valid)\n","                    i_valid = np.array(i_valid)\n","\n","                    loss, accuracy, mae,prediction,y = dev_step(u_valid, i_valid, userid_valid, itemid_valid, y_valid,writer=dev_summary_writer)\n","                    loss_s = loss_s + len(u_valid) * loss\n","                    accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n","                    mae_s = mae_s + len(u_valid) * mae\n","\n","                rmse = np.sqrt(accuracy_s/test_length)  #baraye hameye valid ha \n","                mae = mae_s / test_length\n","                if best_rmse > rmse:              #age koochiktar peyda kardi jaygozin kon\n","                    best_rmse = rmse\n","                if best_mae > mae:\n","                    best_mae = mae\n","        datetime_ist = datetime.now(IST) \n","        time_str= datetime_ist.strftime('%H:%M:%S')    \n","        print (time_str,\" epoch_\"+str(epoch)+\" ended.\\n\" +\"Evaluation:\")\n","\n","        print (\"train:rmse= {:g}, mae= {:g}\".format(train_rmse/total_batch_train , train_mae/total_batch_train))   #bad az hameye batches     \n","        train_rmse =0\n","        train_mae = 0\n","        loss_s = 0\n","        accuracy_s = 0\n","        mae_s = 10\n","\n","        for batch_num in range(total_batch_test):\n","            start_index = batch_num * batch_size\n","            end_index = min((batch_num + 1) * batch_size, test_length)\n","            data_test = test_data[start_index:end_index]\n","\n","            userid_valid, itemid_valid, y_valid = zip(*data_test)\n","            u_valid = []\n","            i_valid = []\n","            for i in range(len(userid_valid)):\n","                u_valid.append(u_text[userid_valid[i][0]])\n","                i_valid.append(i_text[itemid_valid[i][0]])\n","            u_valid = np.array(u_valid)\n","            i_valid = np.array(i_valid)\n","\n","            loss, accuracy, mae,prediction,y = dev_step(u_valid, i_valid, userid_valid, itemid_valid, y_valid,writer=dev_summary_writer)\n","\n","            loss_s = loss_s + len(u_valid) * loss\n","            accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n","            mae_s = mae_s + len(u_valid) * mae\n","        print (\"valid:rmse= {:g}, mae= {:g}\".format(np.sqrt(accuracy_s / test_length), mae_s / test_length))\n","        \n","        #                 path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n","#                 print(\"Saved model checkpoint to {}\\n\".format(path))\n","        rmse = np.sqrt(accuracy_s / test_length)\n","        mae = mae_s / test_length\n","        if best_rmse > rmse:\n","            best_rmse = rmse\n","        if best_mae > mae:\n","            best_mae = mae\n","\n","        if ((epoch+1)%10==0 and epoch>1):\n","            saved_path = saver.save(sess, '/content/drive/My Drive/dataset/chekpoint_music/new/music', global_step=global_step)\n","            print('model saved in {}'.format(saved_path))\n","       \n","    print ('best rmse:', best_rmse)\n","    print ('best mae:', best_mae)\n","\n","#163939 moheme\n","#104515 moheme"],"execution_count":8,"outputs":[{"output_type":"stream","text":["input user shape: (?, 9, 294)\n","self.embedded_users.shape (?, 9, 294, 100)\n","self.embedded_item.shape (?, 16, 295, 100)\n","user jadid shape:  (?, 294, 100)\n","item jadid shape:  (?, 295, 100)\n","rnn_outputs_user shape (TensorShape([Dimension(294), Dimension(100)]), TensorShape([Dimension(294), Dimension(100)]))\n","rnn_outputs_item shape (TensorShape([Dimension(295), Dimension(100)]), TensorShape([Dimension(295), Dimension(100)]))\n","user_dropOut (?, 294, 100)\n","item_dropOut (?, 295, 100)\n","flat output user: (?, 2646, 100)\n","flat output item: (?, 4720, 100)\n","attention_score_user (?, 2646, 1)\n","attention_score_item (?, 4720, 1)\n","user_feature vector: (?, 100)\n","item_feature vector: (?, 100)\n","Xu+qu (?, 32)\n","Yi+pi (?, 32)\n","FM  (?, 32)\n","score:  (?, 1)\n","self.u_bias (?, 1)\n","predictions (?, 1)\n","total_batch_num:  160 \n","\n","17:56:06  epoch_0 started... \n","18:00:27  epoch_0 ended.\n","Evaluation:\n","train:rmse= 1.36711, mae= 1.10408\n","valid:rmse= 1.234, mae= 0.971679\n","18:00:34  epoch_1 started... \n","18:04:53  epoch_1 ended.\n","Evaluation:\n","train:rmse= 1.12868, mae= 0.89587\n","valid:rmse= 1.18182, mae= 0.970298\n","18:05:00  epoch_2 started... \n","18:09:19  epoch_2 ended.\n","Evaluation:\n","train:rmse= 1.02545, mae= 0.809149\n","valid:rmse= 1.12693, mae= 0.887951\n","18:09:27  epoch_3 started... \n","18:13:45  epoch_3 ended.\n","Evaluation:\n","train:rmse= 0.946951, mae= 0.746246\n","valid:rmse= 1.11063, mae= 0.88648\n","18:13:52  epoch_4 started... \n","18:18:10  epoch_4 ended.\n","Evaluation:\n","train:rmse= 0.892689, mae= 0.702983\n","valid:rmse= 1.10734, mae= 0.855792\n","18:18:18  epoch_5 started... \n","18:22:36  epoch_5 ended.\n","Evaluation:\n","train:rmse= 0.85472, mae= 0.668398\n","valid:rmse= 1.11443, mae= 0.850657\n","18:22:43  epoch_6 started... \n","18:27:02  epoch_6 ended.\n","Evaluation:\n","train:rmse= 0.825888, mae= 0.643661\n","valid:rmse= 1.10228, mae= 0.847706\n","18:27:09  epoch_7 started... \n","18:31:27  epoch_7 ended.\n","Evaluation:\n","train:rmse= 0.796143, mae= 0.618064\n","valid:rmse= 1.10419, mae= 0.840522\n","18:31:35  epoch_8 started... \n","18:35:54  epoch_8 ended.\n","Evaluation:\n","train:rmse= 0.768115, mae= 0.590781\n","valid:rmse= 1.09671, mae= 0.854681\n","18:36:01  epoch_9 started... \n","18:40:20  epoch_9 ended.\n","Evaluation:\n","train:rmse= 0.751276, mae= 0.575696\n","valid:rmse= 1.09563, mae= 0.846523\n","model saved in /content/drive/My Drive/dataset/chekpoint_music/new/music-1600\n","18:40:28  epoch_10 started... \n","18:44:47  epoch_10 ended.\n","Evaluation:\n","train:rmse= 0.728039, mae= 0.556387\n","valid:rmse= 1.09612, mae= 0.83795\n","18:44:54  epoch_11 started... \n","18:49:14  epoch_11 ended.\n","Evaluation:\n","train:rmse= 0.712131, mae= 0.5396\n","valid:rmse= 1.09773, mae= 0.830044\n","18:49:21  epoch_12 started... \n","18:53:40  epoch_12 ended.\n","Evaluation:\n","train:rmse= 0.701531, mae= 0.527298\n","valid:rmse= 1.10566, mae= 0.841762\n","18:53:48  epoch_13 started... \n","18:58:08  epoch_13 ended.\n","Evaluation:\n","train:rmse= 0.68827, mae= 0.515794\n","valid:rmse= 1.09416, mae= 0.842872\n","18:58:16  epoch_14 started... \n","19:02:36  epoch_14 ended.\n","Evaluation:\n","train:rmse= 0.678709, mae= 0.505142\n","valid:rmse= 1.09992, mae= 0.829997\n","best rmse: 1.0863934787705685\n","best mae: 0.8195454567670822\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0ba-tMlnNafV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":874},"executionInfo":{"status":"error","timestamp":1595860565561,"user_tz":-270,"elapsed":6255,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"fc4dedd6-e3a6-491c-93fd-80e5454baaf3"},"source":["w#resume training\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","tf.reset_default_graph()\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from datetime import datetime \n","import pytz \n","  \n","IST = pytz.timezone('Asia/Tehran') \n","datetime_ist = datetime.now(IST) \n","current_time = datetime_ist.strftime('%H%M%S')\n","import time\n","\n","\n","# session_conf = tf.ConfigProto(allow_soft_placement=allow_soft_placement, log_device_placement=log_device_placement)\n","# session_conf.gpu_options.allow_growth = True\n","# sess = tf.Session(config=session_conf)\n","with tf.Session() as sess: \n","    saver = tf.train.import_meta_graph('/content/drive/My Drive/dataset/chekpoint_music/new/music-1600.meta')\n","    saver.restore(sess,tf.train.latest_checkpoint('/content/drive/My Drive/dataset/chekpoint_music/new'))\n","\n","    \n","    deep = NARRE( review_num_u=review_num_u, review_num_i=review_num_i,  review_len_u=review_len_u,  review_len_i=review_len_i,\n","        user_num=user_num, item_num=item_num, num_classes=1, user_vocab_size=len(vocabulary_user), item_vocab_size=len(vocabulary_item),\n","        embedding_size=embedding_dim,  embedding_id=32,l2_reg_lambda=l2_reg_lambda, attention_size=32,  n_latent=32)\n","    \n","    global_step = tf.Variable(1600, name=\"global_step\", trainable=False)\n","    # optimizer = tf.train.AdagradOptimizer(learning_rate=0.01, initial_accumulator_value=1e-8).minimize(deep.loss)\n","    optimizer =tf.train.AdamOptimizer(0.006) # tf.train.AdamOptimizer(0.002, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(deep.loss)\n","    grads_and_vars = optimizer.compute_gradients(deep.loss)\n","    train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n","    saver.load\n","\n","    grad_summaries = []\n","    for g, v in grads_and_vars:\n","        if g is not None:\n","            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n","            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n","            grad_summaries.append(grad_hist_summary)\n","            grad_summaries.append(sparsity_summary)\n","    grad_summaries_merged = tf.summary.merge(grad_summaries)\n","\n","    # out_dir = os.path.join('/content/drive/My Drive/dataset/chekpoint_music/run', current_time)\n","    out_dir='run/'+current_time\n","    # Summaries for loss and accuracy\n","    loss_summary = tf.summary.scalar(\"loss\", deep.loss)\n","    acc_summary = tf.summary.scalar(\"RMSE\", deep.RMSE)\n","    MAE_summary = tf.summary.scalar(\"MAE\", deep.MAE)\n","    # Train Summaries\n","    train_summary_op = tf.summary.merge([loss_summary, acc_summary,MAE_summary,grad_summaries_merged])\n","    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n","    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n","\n","    # Dev summaries\n","    dev_summary_op = tf.summary.merge([loss_summary, acc_summary,MAE_summary])\n","    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n","    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n","    # sess.run(tf.global_variables_initializer())\n","    # sess.run(train_op)\n","    saver = tf.train.Saver()     \n","\n","    # load any vectors from the word2vec         \n","    sess.run(deep.W1.assign(initW1))   \n","    sess.run(deep.W2.assign(initW2))\n","    epoch = 10\n","    best_mae =5\n","    best_rmse =5\n","    train_mae = 0\n","    train_rmse = 0       \n","    batch_size=100\n","\n","    total_batch_train = int(train_length / batch_size) #40\n","    total_batch_test = int(test_length / batch_size)  #5  \n","    print(\"total_batch_num: \",total_batch_train,\"\\n\")\n","    for epoch in range(epoch):\n","        # Shuffle the data at each epoch\n","        shuffle_indices = np.random.permutation(np.arange(train_length))\n","        shuffled_data = train_data[shuffle_indices]\n","        datetime_ist = datetime.now(IST) \n","        time_str= datetime_ist.strftime('%H:%M:%S')\n","        print(time_str,\" epoch_\"+str(epoch),\"started... \")            \n","                \n","        for batch_num in range(total_batch_train):\n","            start_index = batch_num * batch_size\n","            end_index = min((batch_num + 1) * batch_size, train_length)\n","            data_train = shuffled_data[start_index:end_index]\n","            uid, iid, y_batch = zip(*data_train) # reuid hamun reid_user\n","            u_batch = []\n","            i_batch = []\n","            for i in range(len(uid)):\n","                u_batch.append(u_text[uid[i][0]])\n","                i_batch.append(i_text[iid[i][0]])\n","            u_batch = np.array(u_batch)\n","            i_batch = np.array(i_batch)\n","            t_rmse, t_mae, fm,Train_prediction,y = train_step(u_batch, i_batch, uid, iid, y_batch,batch_num)\n","            print('after train step')\n","            current_step = tf.train.global_step(sess, global_step)\n","            train_rmse += t_rmse\n","            train_mae += t_mae\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["input user shape: (?, 9, 294)\n","self.embedded_users.shape (?, 9, 294, 100)\n","self.embedded_item.shape (?, 16, 295, 100)\n","user jadid shape:  (?, 294, 100)\n","item jadid shape:  (?, 295, 100)\n","rnn_outputs_user shape (TensorShape([Dimension(294), Dimension(100)]), TensorShape([Dimension(294), Dimension(100)]))\n","rnn_outputs_item shape (TensorShape([Dimension(295), Dimension(100)]), TensorShape([Dimension(295), Dimension(100)]))\n","user_dropOut (?, 294, 100)\n","item_dropOut (?, 295, 100)\n","flat output user: (?, 2646, 100)\n","flat output item: (?, 4720, 100)\n","attention_score_user (?, 2646, 1)\n","attention_score_item (?, 4720, 1)\n","user_feature vector: (?, 100)\n","item_feature vector: (?, 100)\n","Xu+qu (?, 32)\n","Yi+pi (?, 32)\n","FM  (?, 32)\n","score:  (?, 1)\n","self.u_bias (?, 1)\n","predictions (?, 1)\n","total_batch_num:  160 \n","\n","19:06:02  epoch_0 started... \n","after feed dict\n"],"name":"stdout"},{"output_type":"error","ename":"FailedPreconditionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value global_step_1\n\t [[{{node _retval_global_step_1_0_4}}]]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-c153c9411a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mu_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mi_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mt_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after train step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-a6961f47baad>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(u_batch, i_batch, uid, iid, y_batch, batch_num)\u001b[0m\n\u001b[1;32m     13\u001b[0m     _, step,summaries, loss, RMSE, MAE, fm,prediction,y = sess.run(\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         , feed_dict)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# time_str = datetime.datetime.now().strftime(\"%X\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value global_step_1\n\t [[{{node _retval_global_step_1_0_4}}]]"]}]},{"cell_type":"code","metadata":{"id":"dFkz_CmvMUxl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593201349449,"user_tz":-270,"elapsed":699,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"5aef2959-2725-490c-8a18-dde2e609bf8a"},"source":["saved_path = saver.save(sess, '/content/drive/My Drive/dataset/chekpoint_music/new/musicLast')\n","print('model saved in {}'.format(saved_path))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["model saved in /content/drive/My Drive/dataset/chekpoint_music/new/musicLast\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c1r896npQ4cH","colab_type":"code","colab":{}},"source":["from sklearn.metrics import precision_score,recall_score,precision_recall_curve\n","\n","precision= precision_score(y,prediction.astype(int),average='weighted')\n","recall=recall_score(y,prediction.astype(int),average='weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZpBM3unByeX","colab_type":"code","colab":{}},"source":["!kill 366"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"swE95HDG4bVR","colab_type":"code","colab":{}},"source":["%reload_ext tensorboard\n","\n","%tensorboard --logdir='/content/drive/My Drive/dataset/chekpoint_music/run/104515/summaries/train'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ws8utZG2OG5B","colab_type":"code","colab":{}},"source":["vars_list = tf.train.list_variables('/content/drive/My Drive/dataset/chekpoint_music/new') #just variables \n","vars_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pKBwaYBW6q3g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595586498828,"user_tz":-270,"elapsed":18532,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"9348a6aa-e387-40ac-adb6-03bf4a517e9c"},"source":["#test model\n","\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","tf.reset_default_graph()\n","\n","pkl_file = open('/content/drive/My Drive/dataset/music/title/music.train', 'rb')\n","train_data = pickle.load(pkl_file)\n","\n","\n","with tf.Session() as sess:    \n","    saver = tf.train.import_meta_graph('/content/drive/My Drive/dataset/chekpoint_music/new/music-6880.meta')\n","    saver.restore(sess,tf.train.latest_checkpoint('/content/drive/My Drive/dataset/chekpoint_music/new'))\n","\n","    graph = tf.get_default_graph()\n","    input_u = graph.get_tensor_by_name('input_u:0')\n","    input_i = graph.get_tensor_by_name('input_i:0')\n","    input_y = graph.get_tensor_by_name('input_y:0')\n","    input_uid = graph.get_tensor_by_name('input_uid:0')\n","    input_iid = graph.get_tensor_by_name('input_iid:0')\n","    prediction = graph.get_tensor_by_name(\"prediction/predictions:0\")\n","    score= graph.get_tensor_by_name(\"attention_user/attention_score_user:0\")\n","    \n","    data_train = train_data[:100]\n","    uid, iid, y_batch = zip(*data_train) # reuid hamun reid_user\n","    u_batch = []\n","    i_batch = []\n","    for i in range(len(uid)):\n","        u_batch.append(u_text[uid[i][0]])\n","        i_batch.append(i_text[iid[i][0]])\n","    u_batch = np.array(u_batch)\n","    i_batch = np.array(i_batch)\n","\n","    feed_dict = {                         #placeholder haro meghdardehi sabet mikone\n","        input_u: u_batch,\n","        input_i: i_batch,\n","        input_uid: uid,\n","        input_iid: iid,\n","        input_y: y_batch,\n","    }\n","    prediction = sess.run(prediction,feed_dict)\n","    attention_score=sess.run(score,feed_dict)\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/My Drive/dataset/chekpoint_music/new/music-6880\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fEaOHyAK2NYI","colab_type":"code","colab":{}},"source":["print(uid[0][0])\n","print(iid[0][0])\n","print(y_batch[0][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hE7UVTSsKpHB","colab_type":"code","colab":{}},"source":["df=pd.read_csv(\"/content/drive/My Drive/dataset/music/title/music_processed.csv\")\n","# df.loc[df['user_id']==2426]\n","df[\"reviews\"][17839]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C0kwMZ0Ozggt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595587026569,"user_tz":-270,"elapsed":1619,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"f09efbd7-28e5-455b-a9ba-1ef35fda20d9"},"source":["# np.where(attention_score[0] == np.amax(attention_score[0]))\n","np.amax(attention_score[0][:10])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.000978405"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"ZjZ36a6ZWtGi","colab_type":"code","colab":{}},"source":["print(attention_score[0][:10].round(decimals=10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hg2zJuLxNc9L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595526395483,"user_tz":-270,"elapsed":731,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"c1c1bfb1-b696-4e3b-dde3-7f5a6465bf69"},"source":["#number of wrong answers\n","np.sum(np.array(prediction).round() != y_batch)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"yNmvwxBqRYy6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1593263661750,"user_tz":-270,"elapsed":2147,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"4d763163-4dbc-4573-9207-f307edb5286c"},"source":["print(*prediction[:5])\n","print(*y_batch[:5])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[5.3418446] [5.319431] [3.9605584] [4.451573] [5.1865077]\n","[5.] [5.] [5.] [4.] [5.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z201EXPZYABC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593264750837,"user_tz":-270,"elapsed":2047,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"1b87e90f-cfb5-4ce0-8f63-859576ada8c0"},"source":["#recommendation for a user\n","uid, iid, y_batch = zip(*train_data[:100])\n","for i in range(100):\n","  if(uid[i][0]==2426):\n","    if(prediction[i]>4.0):\n","      print(iid[i][0] )\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1563\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oYGKx0MB7hcW","colab_type":"code","colab":{}},"source":["import pandas as pd\n","title= pd.read_csv(\"/content/drive/My Drive/dataset/music/title/music_processed.csv\")\n","title.head()\n","for a in title.values:\n","  if (a[1]==1563):\n","    print(a[4])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-vRbvdB3JGg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593212780605,"user_tz":-270,"elapsed":692,"user":{"displayName":"narges farrokhshad","photoUrl":"","userId":"09411964733981171008"}},"outputId":"7f25118f-2db7-42a4-b02a-59afaf47e38a"},"source":["df = pd.merge(data2, dfTitle, how='right', on=['item_id'])\n","df=df[['user_id','item_id','reviews','Ratings','title']]\n","df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(58490, 5)"]},"metadata":{"tags":[]},"execution_count":13}]}]}